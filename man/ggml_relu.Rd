% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/operations.R
\name{ggml_relu}
\alias{ggml_relu}
\title{ReLU Activation (Graph)}
\usage{
ggml_relu(ctx, a)
}
\arguments{
\item{ctx}{GGML context}

\item{a}{Input tensor}
}
\value{
Tensor representing the ReLU operation
}
\description{
Creates a graph node for ReLU (Rectified Linear Unit) activation: max(0, x)
}
